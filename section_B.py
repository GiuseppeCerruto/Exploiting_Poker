import os
import utils
import torch
import itertools

import rlcard
from rlcard.agents import RandomAgent
from rlcard.utils import get_device, set_seed, tournament, reorganize
from rlcard.models.leducholdem_rule_models import LeducHoldemRuleAgentV1
from rlcard.models.limitholdem_rule_models import LimitholdemRuleAgentV1


def train(args):
    # Check whether gpu is available
    device = get_device()

    # Seed numpy, torch, random
    set_seed(args['seed'])

    # Create list containing the name of the algorithms for training
    algorithms = ['DQN', 'PDQN', 'NFSP']

    # Create list containing number of episodes to train the agents
    num_episodes_list = [100000, 100000, 100000]

    # Create game lists
    games = ['leduc-holdem', 'limit-holdem']

    utils.create_dirs('section_B', 'expertsRLC')

    for game in games:
        print(f'Learning on {game} started...\n')
        agents_performance = []
        for algorithm, num_episodes in zip(algorithms, num_episodes_list):
            print(f'{algorithm} training started...\n')

            # Make the environment with seed and num_players
            env = rlcard.make(game, config={'seed': args['seed'], 'num_players': 2})

            # Create all agents in the game
            agent = utils.get_agent(game, algorithm, env, device)
            game_agents = [agent]
            if game == 'leduc-holdem':
                game_agents.append(LeducHoldemRuleAgentV1())
            else:
                game_agents.append(LimitholdemRuleAgentV1())
            env.set_agents(game_agents)

            rewards_list = []
            step, rew = 0, tournament(env, args['num_games'])[0]
            rewards_list.append((step, rew))
            while env.timestep < num_episodes:

                # Generate data from the environment
                trajectories, payoffs = env.run(is_training=True)

                # Reorganaize the data to be state, action, reward, next_state, done
                trajectories = reorganize(trajectories, payoffs)

                # Feed transitions into agent memory, and train the agent
                # Here, we assume that DQN always plays the first position
                # and the other players play randomly (if any)
                for ts in trajectories[0]:
                    agent.feed(ts)

                # Evaluate the performance. Play with random agents.
                if env.timestep % args['evaluate_every'] == 0:
                    step, rew = env.timestep // args['evaluate_every'], tournament(env, args['num_games'])[0]
                    rewards_list.append((step, rew))

            # Save model performances
            agents_performance.append((rewards_list, algorithm))

            # Save agent's model
            save_path = os.path.join('expertsRLC_section_B', f'{game}_{algorithm}_model.pth')
            torch.save(agent, save_path)

            env.reset()

            print(f'{algorithm} training completed!\n')
            print('----------------------------------------------\n')

        print(f'Learning on {game} completed!\n')
        print('**********************************************')
        print('**********************************************\n')
        utils.plot('expertRLC', game, agents_performance, 2, 'plots_section_B')

    print('Everything completed!')


def analyze_performance(ag1, ag2):
    # Check whether gpu is available
    device = get_device()

    # Create list containing the name of the algorithms for training
    algorithms = ['DQN', 'PDQN', 'NFSP']

    # Create game lists
    games = ['leduc-holdem', 'limit-holdem']

    # Create list containing number of players tested

    print(f'\n{ag1} vs {ag2} tournaments started!\n')

    game_perf = []
    for game in games:
        print(f'{game[0].upper() + game[1:]}')
        algo_perf = []
        for algorithm in algorithms:
            print(f'    {algorithm} agent...')
            players_perf = []

            # Make the environment with seed and num_players
            env = rlcard.make(game, config={'seed': 47, 'num_players': 2})

            # Load the agent model
            if ag1 == 'baselineRLC':
                if game == 'leduc-holdem':
                    agent1 = LeducHoldemRuleAgentV1()
                else:
                    agent1 = LimitholdemRuleAgentV1()
            else:
                model_path = os.path.join(ag1 + '_section_B', f'{game}_{algorithm}_model.pth')
                agent1 = torch.load(model_path, map_location=device)
                agent1.set_device(device)

            if ag2 == 'baselineRLC':
                if game == 'leduc-holdem':
                    agent2 = LeducHoldemRuleAgentV1()
                else:
                    agent2 = LimitholdemRuleAgentV1()
            else:
                model_path = os.path.join(ag2 + '_section_B', f'{game}_{algorithm}_model.pth')
                agent2 = torch.load(model_path, map_location=device)
                agent2.set_device(device)

            # Set the agents in the environment
            game_agents = [agent1, agent2]

            env.set_agents(game_agents)

            # Get average reward over 1000 tournaments
            rew = tournament(env, 1000)[0]

            # Record performance for a specific algorithm
            algo_perf.append((algorithm, rew))
        print()
        # Record performance for a specific game
        game_perf.append((game, algo_perf))

    print('Everything complete!\n')
    f_name = ag1 + "_vs_" + ag2 + f'_tournament.txt'
    utils.save_base_res_B(game_perf, 'plots_section_B', f_name)


def evaluate():
    agents = ['baselines', 'experts', 'baselineRLC', 'expertsRLC']

    combinations = itertools.combinations(agents, 2)
    for c in combinations:
        analyze_performance(c[0], c[1])


if __name__ == '__main__':
    os.environ["CUDA_VISIBLE_DEVICES"] = 'cuda:0'

    args = {'seed': 43,
            'evaluate_every': 10,
            'num_games': 200
            }

    # Train all the experts agents
    # train(args)

    # Evaluate all the agents
    evaluate()
