import os
import utils
import torch

import rlcard
from rlcard.agents import RandomAgent
from rlcard.utils import get_device, set_seed, tournament, reorganize

def train(args, t_typ):
    # Check whether gpu is available
    device = get_device()
        
    # Seed numpy, torch, random
    set_seed(args['seed'])

    # Create list containing the name of the algorithms for training
    algorithms = ['DQN', 'PDQN', 'NFSP']
    #algorithms = ['NFSP']

    # Create list containing number of episodes to train the agents
    num_episodes_list = [2000, 2000, 2000]

    # Create game lists
    games = [args['game']]

    utils.create_dirs('section_C', f'{t_typ}s')
    print('\nDirectories fixed!\n')

    for game in games:
        print(f'Learning on {game} started...\n')
        agents_performance = []
        for algorithm, num_episodes in zip(algorithms, num_episodes_list):
            print(f'{algorithm} training started...\n')

            # Make the environment with seed and num_players
            env = rlcard.make(game, config={'seed': args['seed'], 'num_players': 2})

            # Create all agents in the game
            agent = utils.get_agent(game, algorithm, env, device)
            if t_typ == 'baseline':
                env.set_agents([agent, RandomAgent(num_actions=env.num_actions)])
            else:
                model_path = os.path.join(f'baselines_section_C', f'{game}_{algorithm}_model.pth')
                new_agent = torch.load(model_path, map_location=device)
                new_agent.set_device(device)
                env.set_agents([agent, new_agent])
            
            rewards_list = []
            step, rew = 0, tournament(env, args['num_games'])[0]
            rewards_list.append((step, rew))
            for ep in range(num_episodes):
                # Generate data from the environment
                trajectories, payoffs = env.run(is_training=True)

                # Reorganaize the data to be state, action, reward, next_state, done
                trajectories = reorganize(trajectories, payoffs)

                # Feed transitions into agent memory, and train the agent
                # Here, we assume that DQN always plays the first position
                # and the other players play randomly (if any)
                for ts in trajectories[0]:
                    agent.feed(ts)

                # Evaluate the performance. Play with random agents.
                if ep % args['evaluate_every'] == 0:
                    step, rew = ep // args['evaluate_every'], tournament(env, args['num_games'])[0]
                    rewards_list.append((step, rew))
            #print(f'\nEnv timestep: {env.timestep}, Agent timestep: {agent.total_t}, Last episode: {ep}')
            # Save model performances
            agents_performance.append((rewards_list, algorithm))

            # Save agent's model
            save_path = os.path.join(f'{t_typ}s_section_C', f'{game}_{algorithm}_model.pth')
            torch.save(agent, save_path)

            env.reset()

            print(f'{algorithm} training completed!\n')
            print('----------------------------------------------\n')
        
        print(f'Learning on {game} completed!\n')
        print('**********************************************')
        print('**********************************************\n')
        utils.plot(t_typ, game, agents_performance, 2, 'plots_section_C')

    print('Everything completed!')


def analize_performance(typ):
    # Check whether gpu is available
    device = get_device()

    # Create list containing the name of the algorithms for training
    algorithms = ['DQN', 'PDQN', 'NFSP']

    # Create game lists
    games = ['blackjack', 'leduc-holdem', 'limit-holdem']
    
    # Create list containing number of players tested
    num_players_list = [2, 3, 4, 5, 6]

    print(f'\n{typ[0].upper() + typ[1:]} tournaments started!\n')
    
    game_perf= []
    for game in games:
        print(f'{game[0].upper() + game[1:]}')
        algo_perf = []
        for algorithm in algorithms:
            print(f'    {algorithm} agent...')
            players_perf = []
            for num_players in num_players_list:
                # Make the environment with seed and num_players
                env = rlcard.make(game, config={'seed': 47, 'num_players': num_players})

                # Load the agent model
                model_path = os.path.join(typ + 's_section_C', f'{game}_{algorithm}_model.pth')
                agent = torch.load(model_path, map_location=device)
                agent.set_device(device)

                # Set the angents in the environment
                game_agents = [agent]
                for _ in range(1, env.num_players):
                    game_agents.append(RandomAgent(num_actions=env.num_actions))
                env.set_agents(game_agents)
                
                # Get average reward over 1000 tournaments
                rew = tournament(env, 10000)[0]
                
                # Record performance for a N-players game
                players_perf.append((num_players, rew))
            # Record performance for a specific algorithm
            algo_perf.append((algorithm, players_perf))
        print()
        # Record performance for a specific game
        game_perf.append((game, algo_perf))

    print('Everything complete!\n')
    f_name = typ + f'_tournaments_{"-".join([str(n) for n in num_players_list])}.txt'
    utils.save_base_res(game_perf, 'plots_section_C', f_name)

def analize_performance2():
    # Check whether gpu is available
    device = get_device()

    # Create list containing the name of the algorithms for training
    algorithms = ['DQN', 'PDQN', 'NFSP']

    # Create game lists
    games = ['blackjack', 'leduc-holdem', 'limit-holdem']
    
    # Create list containing number of players tested
    num_players_list = [2, 3, 4, 5, 6]

    print(f'\nExpert tournaments started!\n')
    
    game_perf= []
    for game in games:
        print(f'{game[0].upper() + game[1:]}')
        algo_perf = []
        for algorithm in algorithms:
            print(f'    {algorithm} agent...')
            players_perf = []
            for num_players in num_players_list:
                # Make the environment with seed and num_players
                env = rlcard.make(game, config={'seed': 47, 'num_players': num_players})

                # Load the agent model
                model_path = os.path.join('experts_section_C', f'{game}_{algorithm}_model.pth')
                agent = torch.load(model_path, map_location=device)
                agent.set_device(device)

                # Set the angents in the environment
                model_path = os.path.join('baselines_section_C', f'{game}_{algorithm}_model.pth')
                game_agents = [agent]
                for _ in range(1, env.num_players):
                    new_agent = torch.load(model_path, map_location=device)
                    new_agent.set_device(device)
                    game_agents.append(new_agent)
                env.set_agents(game_agents)
                
                # Get average reward over 10000 tournaments
                rew = tournament(env, 10000)[0]
                
                # Record performance for a N-players game
                players_perf.append((num_players, rew))
            # Record performance for a specific algorithm
            algo_perf.append((algorithm, players_perf))
        print()
        # Record performance for a specific game
        game_perf.append((game, algo_perf))

    print('Everything complete!\n')
    f_name = 'exp_2_base' + f'_tournaments_{"-".join([str(n) for n in num_players_list])}.txt'
    utils.save_base_res(game_perf, 'plots_section_C', f_name)


if __name__ == '__main__':

    args = {'seed': 43,
            'evaluate_every': 10,
            'num_games': 200,
            'game': 'limit-holdem'} # Use 'blackjack', 'leduc-holdem', and 'limit-holdem'

    # Train all the baseline agents
    #train(args, 'baseline')

    # Analyze baseline parformance
    #analize_performance('baseline')

    # Train all the experts agents
    #train_expert(args)
    #train(args, 'expert')

    # Analyze baseline performance
    #analize_performance('expert')

    # Analyze mixed performance
    analize_performance2()