import os
from matplotlib import pyplot as plt

import rlcard
from rlcard.agents import RandomAgent
from rlcard.agents import DQNAgent
from rlcard.utils import get_device, set_seed, tournament, reorganize
from rlcard.models.leducholdem_rule_models import LeducHoldemRuleAgentV1

def plot(rew_list, mode):
    if mode == "single":
        title = "DQN self-play - SINGLE training"
        png = 'single_training'
    if mode == "double":
        title = "DQN self-play - DOUBLE training"
        png = 'double_training'
    if mode == "singlex2":
        title = "DQN self-play - SINGLE training / double episodes"
        png = 'single_training-double_episodes'

    plt.title(title.translate(str.maketrans("12", "₁₂")))
    plt.xlabel("Time".translate(str.maketrans("1", "₁")))
    plt.ylabel("Reward".translate(str.maketrans("2", "₂")))
    plt.grid(color="#989898", zorder=0)
    times, rews = zip(*rew_list)
    plt.plot(times, rews, zorder=3)
    plt.tight_layout()
    plt.savefig(os.path.join('plots_section_D', png))
    plt.clf()

    return


def train(args, mode):
    # Extract base arguments
    log_dir = args['log_dir']
    seed = args['seed']
    num_episodes = args['num_episodes']
    evaluate_every = args['evaluate_every']
    num_games = args['num_games']
    game = args['game']

    # Check whether gpu is available
    device = get_device()

    current = os.getcwd()
    plots = 'plots_section_D'
    dirs = os.listdir(current)
    if plots not in dirs:
        os.makedirs(plots)

    # Seed numpy, torch, random
    set_seed(seed)

    # Make the environment with seed
    env = rlcard.make(game, config={'seed': seed, 'num_players': 2})
    env_t = rlcard.make(game, config={'seed': seed, 'num_players': 2})

    # Initialize the DQN agent
    agent = DQNAgent(num_actions=env.num_actions,
                     state_shape=env.state_shape[0],
                     mlp_layers=[128, 128],
                     device=device,
                     )

    agents = [agent, agent]
    agents_t = [agent, RandomAgent(num_actions=env_t.num_actions)] #tournament vs random
    # agents_t = [agent, LeducHoldemRuleAgentV1()] #tournament vs expertRLC


    env.set_agents(agents)
    env_t.set_agents(agents_t)

    if mode == "singlex2":
        num_episodes *= 2
        evaluate_every *= 2

    # Start training

    rewards_list = list()
    # while env.timestep < num_episodes:
    for episode in range(num_episodes):
        # Generate data from the environment
        trajectories, payoffs = env.run(is_training=True)

        # Reorganize the data to be state, action, reward, next_state, done
        trajectories = reorganize(trajectories, payoffs)

        # Feed transitions into agent memory, and train the agent
        for ts in trajectories[0]:
            agent.feed(ts)
        if mode == "double":
            for ts in trajectories[1]:
                agent.feed(ts)

        # Evaluate the performance. Play with random agents
        # if env.timestep % evaluate_every == 0:
        if episode % evaluate_every == 0:
            # step, rew = env.timestep // evaluate_every, tournament(env, num_games)[0]
            step, rew = episode, tournament(env_t, num_games)[0]
            rewards_list.append((step, rew))

    # Plot the learning curve
    plot(rewards_list, mode)



if __name__ == '__main__':
    os.environ["CUDA_VISIBLE_DEVICES"] = 'cuda:0'

    args = {'log_dir': 'logs_s1',
            'seed': 43,
            'num_episodes': 10000,
            'evaluate_every': 10,
            'num_games': 200,
            'game': 'leduc-holdem'}

    mode = "single"  #single training (aka only on trajectories[0]
    train(args, mode)

    mode = "double"  #double training (aka both on trajectories[0] and [1]
    train(args, mode)

    mode = "singlex2" #single training with double num_episodes
    train(args, mode)